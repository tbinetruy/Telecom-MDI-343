{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cadre décisionnel et notations\n",
    "\n",
    "On se place dans le cadre de la classification multi-classe, avec les notations habituelles (pour le détails des notations, on pourra consulter l’énoncé du TP sur les $k$-plus proches voisins) : on suppose que les données peuvent être reparties dans $K$ classes. L’ensemble d’apprentissage est de taille $n : D n = \\{(x_i , y_i ), i = 1, ..., n\\}$ contenant les n observations (les $x_i$ ) et leurs étiquettes (les $y_i$). Pour mémoire $x_i = (x_1 , ..., x_p ) > \\in X \\subset \\mathbb{R}^p$ est une observation, et dans le cas bidimensionnel $p = 2$.\n",
    "\n",
    "# Génération artificielle de données\n",
    "\n",
    "On peut utiliser de nouveau les fonctions qui génèrent les données des TP précédents (knn, perceptron notamment). Afin d’afficher quelques jeux de données, vous pouvez aussi utiliser ou modifier les fonctions `plot_2d` ou `plot_2d_simple` du fichier `tp_tree_source.py`.\n",
    "\n",
    "# Arbres de décision - Algorithme CART\n",
    "\n",
    "On pourra consulter [2, Chapitre 9.2] pour plus de détails sur les arbres. La source la plus détaillée\n",
    "sur le sujet étant le livre fondateur [1]. Rappelons ici le fonctionnement d’un arbre de décision (voir aussi les figures en dernière page). Notons qu’on ne considère que des arbres binaires par simplicité : un nœud ne peut avoir que deux enfants, sauf si c’est une feuille, auquel cas il n’en a aucun.\n",
    "\n",
    "On associe à toute partition des données une représentation par arbre. Au départ l’arbre est restreint à\n",
    "un seul nœud, sa racine, qui représente l’espace $\\mathcal{X}$ tout entier. Récursivement, à chaque étape on choisit :\n",
    "— une variable $j \\in \\{1, ..., p\\}$ (parmi les p possibles),\n",
    "— un seuil $\\tau \\in \\mathbb{R}$\n",
    "\n",
    "et l’on partitionne l’espace des variables explicatives X en deux sous-ensembles qui sont représentés par deux nœuds dans l’arbre $G(j, \\tau) = \\{x = (x_1 , ..., x_p )^T \\in \\mathbb{R}^p : x_j < \\tau \\}$ et $D(j, \\tau) = \\{x = (x_1 , ..., x_p)^T \\in \\mathbb{R}^p : x_j \\geq \\tau \\}$. On incrémente donc à chaque étape le nombre de composantes de la partition, et de manière équivalente le nombre de feuilles de l’arbre. On répète le processus jusqu’à atteindre un critère d’arrêt,\n",
    "qui peut être :\n",
    "\n",
    "- le fait que la profondeur de l’arbre dépasse un seuil prescrit,\n",
    "- le fait que l’effectif d’un nœud (i.e., le nombre d’observations qui tombent dans la partition correspondante) est inférieur à un seuil prescrit,\n",
    "- le fait que le nombre de feuilles de l’arbre dépasse un seuil prescrit.\n",
    "- etc.\n",
    "\n",
    "Un exemple visuel d’une telle construction est donné à la Figure 1. Il faut maintenant définir une règle pour décider où l’on doit faire la nouvelle découpe (splitting). Ce choix est crucial et n’est pas unique. Pour cela on utilise une fonction qui mesure “l’impureté”, que l’on note $H$ associée à une partition. On cherche alors la découpe (variable/seuil) qui produit une partition la plus pure possible selon le critère $H$. Mathématiquement il s’agit de résoudre :\n",
    "\n",
    "$$\n",
    "\\operatorname*{argmax}_{j\\in [1,p], \\tau\\in\\mathbb{R}} \\hat{q}_{j,\\tau}H(G(j,\\tau)) + (1-\\hat{q}_{j,\\tau})H(G(j,\\tau))\n",
    "$$\n",
    "\n",
    "où l’on a noté\n",
    "\n",
    "$$\n",
    "\\hat{q}_{j,\\tau} = \\frac\n",
    "    {|\\{ i \\in [1,n] : x_i \\in G(j,\\tau) \\}|}\n",
    "    {|\\{ i' \\in [1,n] : x_{i'} \\in G(j,\\tau) \\cup D(j,\\tau) \\}|}\n",
    "$$\n",
    "\n",
    "la proportion des observations qui tombent dans $G(j, \\tau)$. Noter qu’ici $| · |$ représente le cardinal d’un\n",
    "ensemble. Pour tout ensemble $\\mathbb{R} \\subset \\mathbb{R}^p$ et toute étiquette k on note p b k (R) la proportion d’observations qui ont $k$ comme étiquette (numérotées de 1 à $K$), i.e.,\n",
    "\n",
    "$$\n",
    "\\hat{p}_{k}(R) = \\frac\n",
    "    {|\\{ i \\in [1,n] : x_i \\in R \\mbox{ et } y_i=k \\}|}\n",
    "    {|\\{ i \\in [1,n] : x_{i} \\in R \\}|}\n",
    "$$\n",
    "\n",
    "On considérera dans CART les mesures d’impureté H suivantes :\n",
    "- l’indice de Gini : $\\sum_{k=1}^K \\hat{p}_k(R)(1-\\hat{p}_k(R)$\n",
    "- l’entropie : $-\\sum_{k=1}^K \\hat{p}_k(R)\\log (\\hat{p}_k(R))$\n",
    "\n",
    "# Question 1\n",
    "\n",
    "Dans le cadre de la régression (i.e., quand on cherche à prédire une valeur numérique pour Y et non une classe), proposez une autre mesure d’homogénéité. Justifier votre choix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec `scikit-learn` on peut construire des arbres de décision grâce au package `tree`. On obtient un classifieur avec `tree.DecisionTreeClassifier`.\n",
    "\n",
    "    from sklearn import tree\n",
    "    \n",
    "## Question 2\n",
    "\n",
    "Simulez avec rand_checkers des échantillons de taille $n = 456$ (attention à bien équilibrer les classes). Créez deux courbes qui donnent le pourcentage d’erreurs commises en fonction de la profondeur maximale de l’arbre (une courbe pour Gini, une courbe pour l’entropie). On laissera les autres paramètres à leur valeurs par défaut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
